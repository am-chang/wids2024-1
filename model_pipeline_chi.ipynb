{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import optuna\n",
    "from category_encoders import OneHotEncoder, MEstimateEncoder, CatBoostEncoder, OrdinalEncoder\n",
    "from sklearn import set_config\n",
    "import category_encoders\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, make_scorer, f1_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.preprocessing import FunctionTransformer,StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import auc, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data here\n",
    "train = pd.read_csv('training.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define cat and numeric columns\n",
    "num_cols = train.select_dtypes('number').columns.tolist()\n",
    "target = 'DiagPeriodL90D'\n",
    "cat_cols = [c for c in train.columns if c not in num_cols and c != 'DiagPeriodL90D']\n",
    "num_cols.remove('DiagPeriodL90D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure numeric cols doesn't have numeric categories\n",
    "train[num_cols]\n",
    "\n",
    "# apparently patient_id, patient_zip3 are cat cols\n",
    "cat_cols.extend(['patient_id', 'patient_zip3'])\n",
    "num_cols.remove('patient_id')\n",
    "num_cols.remove('patient_zip3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column here, example of dropping  bmi\n",
    "class DropColumns(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        x_copy = X.copy()        \n",
    "        x_copy = x_copy.drop('bmi',axis=1) # drop column here\n",
    "        return x_copy\n",
    "    \n",
    "# add column here, example of adding an all-one colum \n",
    "class AddColumns(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        x_copy = X.copy()        \n",
    "        # x_copy['all_one'] = np.ones(len(x_copy)) # add column here\n",
    "        return x_copy\n",
    "    \n",
    "# self-define missing value class   \n",
    "class InputCol(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        x_copy = X.copy()   \n",
    "        return x_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cs/Desktop/widsdatathon2024-challenge1/.conda/lib/python3.11/site-packages/sklearn/preprocessing/_function_transformer.py:343: UserWarning: With transform=\"pandas\", `func` should return a DataFrame to follow the set_output API.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ImputeCatCols(d):\n",
    "    df = d.copy()\n",
    "    for c in cat_cols:\n",
    "        df[c] = df[c].fillna('None')\n",
    "    return df\n",
    "\n",
    "num_transformer = Pipeline([('imputer',SimpleImputer(strategy='mean')),\n",
    "                             ('scaler',StandardScaler())])\n",
    "cat_transformer = Pipeline([('imputer',FunctionTransformer(ImputeCatCols))])\n",
    "\n",
    "preprocess = ColumnTransformer([('num',num_transformer,num_cols),\n",
    "                                ('cat',cat_transformer,cat_cols)],\n",
    "                                remainder='passthrough',\n",
    "                                verbose_feature_names_out=False).set_output(transform='pandas')\n",
    "preprocess_catboost= Pipeline([('preprocess',preprocess),\n",
    "                           ('drop',DropColumns())\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_transformer_2= Pipeline([('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "                           ('cat',CatBoostEncoder())])\n",
    "num_transformer_2 = Pipeline([('imputer',SimpleImputer(strategy='most_frequent'))])\n",
    "preprocess_othermodels = ColumnTransformer([('cat',cat_transformer_2,cat_cols),\n",
    "                                            ('num',num_transformer_2,num_cols)\n",
    "                                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, label):\n",
    "    \n",
    "    print('model: {}'.format(label))\n",
    "    X = train.copy()\n",
    "    y = X.pop(target)\n",
    "              \n",
    "    skf = StratifiedKFold(n_splits=5,random_state=SEED, shuffle=True)\n",
    "    \n",
    "    val_predictions = np.zeros(len(train))\n",
    "    score_list = []\n",
    "    for fold, (trx_idx, val_idx) in enumerate(skf.split(X,y)):\n",
    "        X_train = X.iloc[trx_idx]\n",
    "        y_train = y.iloc[trx_idx]\n",
    "        X_val   = X.iloc[val_idx]\n",
    "        y_val   = y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred_train = model.predict_proba(X_train)[:,1]\n",
    "        y_pred_val   = model.predict_proba(X_val)[:,1]\n",
    "        auc_train = roc_auc_score(y_train,y_pred_train)\n",
    "        auc_val   = roc_auc_score(y_val,y_pred_val)\n",
    "        \n",
    "        val_predictions[val_idx] = y_pred_val\n",
    "        score_list.append(auc_val)\n",
    "        print(f'fold: {fold} - AUC Train: {auc_train} - AUC Val {auc_val}') \n",
    "\n",
    "    print(f'AUC MEAN {np.mean(score_list)} - Std: {np.std(score_list)}')  \n",
    "    \n",
    "    return score_list, val_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "\n",
    "params_cat = {'learning_rate': 0.004, \n",
    "              'iterations': 1000,\n",
    "              'max_depth': 5, \n",
    "              'subsample': 0.7401131867566202, \n",
    "              'colsample_bylevel': 0.29684187768021997, \n",
    "              'min_data_in_leaf': 47,\n",
    "              'logging_level': 'Silent'}\n",
    "\n",
    "params_lgb= {'learning_rate': 0.0016,\n",
    "             'subsample': 0.6710494933148675, \n",
    "             'colsample_bytree': 0.7929648706646588, \n",
    "             'num_leaves': 29,\n",
    "             'verbose':-1}\n",
    "\n",
    "\n",
    "params_xbg = {'learning_rate': 0.001, \n",
    "              'max_depth': 6, \n",
    "              'subsample': 0.5281085467708261, \n",
    "              'min_child_weight': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: RF\n",
      "fold: 0 - AUC Train: 0.8502600865669502 - AUC Val 0.8010395615359034\n",
      "fold: 1 - AUC Train: 0.8535540329460948 - AUC Val 0.7977654689928734\n",
      "fold: 2 - AUC Train: 0.8528011997009787 - AUC Val 0.8078609986504722\n",
      "fold: 3 - AUC Train: 0.8538171445651039 - AUC Val 0.7887176158173861\n"
     ]
    }
   ],
   "source": [
    "score_list, oof_list= pd.DataFrame(), pd.DataFrame()\n",
    "models = [          \n",
    "        #    ('catBoost',make_pipeline(preprocess_catboost,\n",
    "        #                             CatBoostClassifier(cat_features=cat_cols,\n",
    "        #                                                **params_cat,\n",
    "        #                                                random_state=SEED))),                                       \n",
    "                            \n",
    "           ('RF',make_pipeline(preprocess_othermodels,\n",
    "                               RandomForestClassifier(n_estimators=200,\n",
    "                                                      random_state=SEED,\n",
    "                                                      min_samples_leaf=92,\n",
    "                                                      max_features=1.0))),\n",
    "           ('Extratrees',make_pipeline(preprocess_othermodels,\n",
    "                                       ExtraTreesClassifier(n_estimators=300,\n",
    "                                                           random_state=SEED,\n",
    "                                                           min_samples_leaf=46,\n",
    "                                                           max_features=1.0))),\n",
    "           ('XGB',make_pipeline(preprocess_othermodels,\n",
    "                                 XGBClassifier(**params_xbg,random_state=SEED)))                                        \n",
    "\n",
    "            \n",
    "        ]\n",
    "for label, model in models:\n",
    "    score_list[label], oof_list[label] = score_model(model,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting from all models\n",
    "w = RidgeClassifier().fit(oof_list,train.DiagPeriodL90D).coef_[0]\n",
    "voter = VotingClassifier(models, weights = w, voting = 'soft')\n",
    "\n",
    "X = train.copy()\n",
    "y = X.pop('DiagPeriodL90D')      \n",
    "                       \n",
    "voter.fit(X,y)\n",
    "voter.predict_proba(test)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
